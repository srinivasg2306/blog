{
  
    
        "post0": {
            "title": "Matrix Factorization Method for Machine Learning Problems",
            "content": "Concept of Vectors and Matrices . Mathematically, A vector is represented by a single dimension array of numbers. Example: V = [25, 3, 46, 30, 33]. This makes no sense to a Machine Learning practitioner until it can be related to something real. Let us say the numbers represents certain data of an individual - Age, years of Work experience, Annual Salary(in 000 USD), Average Expenses per year (in 000 USD), % of Annual salary saved per year. If such data are collected for multiple individuals and stacked up one over the other, then we have an array of vectors which is called a Matrix. . From a Linear Algebra perspective, a Matrix can represent one of the two: . Systems of Linear Equations - Matrix operations can be used to represent a set of linear equations. This helps us solve the equations, define inverse of matrices. | Linear Mapping - Matrices can be used to define Linear Mappings. Linear mapping define transformation actions that can be performed on Vectors. Examples of Vector Transformations that fall into this bucket are: Rotation, Stretching along specific axes, change of basis | Matrix Decomposition . Let us take an example of a Matrix that represents a set of Vectors - i.e . Data. Let us assume that the Data consists of rows - which represents users, and columns which represents products that they have purchased. So we have a user-product matrix. The matrix maps each user to each product. If a user Ui has purchased product Vj then the cell ij will contain the number of items purchased. If a user has never purchased a product, then the corresponding cell will have a 0. Please note that at this stage, we just have Sales Data arranged in a certain way (in the form of matrix). This does not give any additional information. So how do we break it down into chunks of comprehendable pieces? Enter Factorization! . Concept of Matrix Factorization . Suppose you had a number - say 28, and you want to understand it better. What do you do? You can say 28 is 7 times 4, which is 7 times 2 times 2 (7 x 2 x 2). This tells us that 7 and 2 are latent in the number 28. What happens if we apply factorization to Matrices? By extending the logic, we must be able to understand the latent factors of matrices. The process of decomposing a matrix into certain factors of interpretable matrices is known as Matrix Factorization or Matrix Decomposition . Matrix Decomposition Methods . There are two ways to decompose matrices - Eigen Decomposition, Singular Value Decomposition. Both methodologies are similar. The fundamental difference is that Eigen decomposition can only be done on square matrices. Whereas Singular Value Decomposition can be done on any matrix and is generic in a sense. Regardless, at the core of Matrix Decomposition method is identifying values that capture key information about the parent matrix. . Eigen Decomposition . Let $ A = begin{pmatrix} 1 &amp; 2 &amp; 3 3 &amp; 2 &amp; 7 6 &amp; 5 &amp; 4 end{pmatrix} $ . A Square matrix can be decomposed into Eigen values and Eigen vectors as follows . Let $X1, X2, X3.. Xn$ be Eigen Vectors of a matrix A, such that $AX1 = L1X1, AX2 = LX2, ..AXn = LXn,$ where L1, L2, L3,..Ln are Eigen values | Then Matrix $P = [X1, X2, X3... Xn]$ arranged in a columnar fashion, D = Diagonal matrix of $[L1, L2, .. Ln]$, $P^-1$ - inverse of P | P is matrix of eigen vectors, D is eigenvalues arranged in a diagonal matrix, and $P^-1$ is inverse of matrix of eigen vectors | It can be Mathematically proven that $A = P .D .P^-1$ (Dot products) | Eigen values and Eigen vectors capture information about a matrix. The first eigen vector and first eigen value captures highest information about the matrix. Similarly, second eigen vector and second eigen value captures 2nd highest information so on... | This technique is useful in dimensional reduction/compression | . Post Eigen decomposition of A, we have the following . $ P= begin{pmatrix} -0.32703016 &amp; -0.56731925 &amp; -0.05408783 -0.63306085 &amp; 0.81351676 &amp; -0.7949926 -0.70163042 &amp; -0.12782546 &amp; 0.60420301 end{pmatrix} $ . $ D = begin{pmatrix} 11.30795739 &amp; 0 &amp; 0 0 &amp; -1.19198827 &amp; 0 0 &amp; 0 &amp; -3.11596912 end{pmatrix} $ . $ P^-1 = begin{pmatrix} -0.56004896 &amp; -0.50227981 &amp; -0.71102023 -1.35059073 &amp; 0.33832307 &amp; 0.32425164 -0.93608812 &amp; -0.51169638 &amp; 0.8979998 end{pmatrix} $ . An illustration of Eigen Values and Eigen Vectors in action - Google Page Rank Algorithm. . Here is the URL to the page http://pi.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture3/lecture3.html . import numpy as np import pandas as pd . A = np.array([[1,2,3],[3,2,7],[6,5,4]]) A.shape . (3, 3) . D,P = np.linalg.eig(A) . P . array([[-0.32703016, -0.56731925, -0.05408783], [-0.63306085, 0.81351676, -0.7949926 ], [-0.70163042, -0.12782546, 0.60420301]]) . P_inv = np.linalg.inv(P) . P_inv . array([[-0.56004896, -0.50227981, -0.71102023], [-1.35059073, 0.33832307, 0.32425164], [-0.93608812, -0.51169638, 0.89799982]]) . D . array([11.30795739, -1.19198827, -3.11596912]) . Okay, Eigen decomposition works when the matrix is a square matrix. But in most real-life scenarios, the matrices are not square. Therefore, Eigen decomposition is not useful in many cases. So, what do we decompose non-square matrices? Singular Vector Decomposition is the answer to this problem. . Any $A_{mxn}$ Matrix can be decomposed as . $A_{mxn} = U_{mxm}. S_{mxn}. V^T_{nxn}$ . Where . S - Singular matrix which is a diagonal matrix | U - Left Singular matrix | $V^T$ - Right Singular matrix | . The diagonal values in Matrix S is called Singular Matrix and the Singular values arranged in the descending order from top-left . SVD is used in a number of ways in Machine learning including solving systems of linear equations, curve fitting etc. Substituting a matrix with its SVD approximation reduces calculation as well as loss. Here, we describe how this is achieved Let A be a user item matrix (4 users and 5 items) as shown below. The Values in the table describe the quantities purchased . Let us see SVD in action through an Example . Let A be a user item matrix (4 users and 5 items) as shown below. The Values in the table describe the quantities purchased . Cat/User U1 U2 U3 U4 . Formal_Shirt | 2 | 0 | 3 | 4 | | . Trouser | 1 | 0 | 2 | 2 | | . T_shirt | 1 | 2 | 0 | 1 | | . Jeans | 1 | 2 | 0 | 0 | | . Blazer | 0 | 0 | 1 | 1 | | . Let us get started with Decomposition . $ A = begin{pmatrix} 2 &amp; 0 &amp; 3 &amp; 4 1 &amp; 0 &amp; 2 &amp; 2 1 &amp; 2 &amp; 0 &amp; 1 1 &amp; 2 &amp; 0 &amp; 0 0 &amp; 0 &amp; 1 &amp; 1 end{pmatrix} $ . SVD Decomposition results in the following matrices . $ U= begin{pmatrix} 0.83572573 &amp; -0.14515127 &amp; -0.29646439 &amp; 0.26000475 &amp; 0.35355339 -0.46197483 &amp; -0.11449508 &amp; 0.52286834 &amp; 0.00887863 &amp; -0.70710678 -0.19999168 &amp; 0.67730305 &amp; -0.50049805 &amp; -0.35463447 &amp; -0.35355339 -0.08661448 &amp; 0.70353837 &amp; 0.55121115 &amp; 0.26210298 &amp; 0.35355339 -0.20160113 &amp; -0.11007421 &amp; 0.29049188 &amp; -0.85898494 &amp; 0.35355339 end{pmatrix} $ . $ S= begin{pmatrix} 6.41687031 &amp; 0 &amp; 0 &amp; 0 0 &amp; 3.03943637 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0.57698562 &amp; 0 0 &amp; 0 &amp; 0 &amp; 0.5026824 0 &amp; 0 &amp; 0 &amp; 0 end{pmatrix} $ . $ V^T = begin{pmatrix} -0.37713595 &amp; -0.08932896 &amp; -0.56612146 &amp; -0.72752684 0.32112657 &amp; 0.90861676 &amp; -0.25482297 &amp; -0.07974058 -0.03353176 &amp; 0.17578633 &amp; 0.77443073 &amp; -0.60682109 0.86805635 &amp; -0.3681509 &amp; -0.12177355 &amp; -0.31002306 end{pmatrix} $ . Let us calculate the first low rank matrix (Reconstructing) . $U_1.S_1.V_1^T$ . $ Rank 1 = begin{pmatrix} 2.022483 &amp; 0.479048 &amp; 3.035964 &amp; 3.901540 1.117994 &amp; 0.264810 &amp; 1.678229 &amp; 2.156704 0.483986 &amp; 0.114638 &amp; 0.726515 &amp; 0.933650 0.209610 &amp; 0.049648 &amp; 0.314647 &amp; 0.404355 0.487881 &amp; 0.115560 &amp; 0.732362 &amp; 0.941164 end{pmatrix} $ . If you add $U_2.S_2.V_2^T$to Rank 1 approximation then we get Rank 2 Matrix . $ Rank 2= begin{pmatrix} 1.880809 &amp; 0.078187 &amp; 3.148387 &amp; 3.936720 1.006242 &amp; -0.051389 &amp; 1.766907 &amp; 2.184454 1.145064 &amp; 1.985134 &amp; 0.201932 &amp; 0.769495 0.896294 &amp; 1.992598 &amp; -0.230256 &amp; 0.233841 0.380444 &amp; -0.188430 &amp; 0.817617 &amp; 0.967842 end{pmatrix} $ . This is already close to the original matrix. Rank, 3 and 4 will of course add more information to it. However, from computation perspective, it is much easier to multiply 5x2, 2x2, and 2x4 matrices than trying to crunch the original 5x4 matrix . How does this reduce computation? Note that a matrix of Size mxn is replaced with a smaller matrices of mxp, pxn- where p is rank of the singular matrix. P can also be seen as the number of latent factors . In our example we saw 5x4 matrix being replaced with one 5x2 matrix and another 2x4 matrix in addition to two real numbers. In real-life situations, matrices of several thousands of rows and columns can be reduced to ones with few hundreds. Storing such matrices require less memory and information is retained as well. . The Rank P approximation can also be related to the number of latent factors. In fact, the Rank P is also equal to the number of latent factors. Let us delve a little deeper into the example we have taken. . In Matrix U above, the first cell of the first column (-0.83) indicates that formal_shirt is unique and has its own factor. Similarly, the 3rd and 4th cell in the second column are close to each other and have relatively large values. These values correspond to Jeans and T-Shirts. This indicates that Jeans and T-Shirts are part of same cluster (most likely casual clothing). The other columns, even though giving some information about how products might be related, are not significant because the singular values are much lower. . Similarly in Matrix $V^T$ Users 3,4 are close to each other in their tastes (Some one that buys formals and casuals) as shown in the third and fourth cells of 1st row. User 2 is unique in his taste and hence has a value of 0.90 and no other users are close to this user as indicated in 2nd cell of Row 2 in the Matrix. The last two rows, although, giving some information about the users, are not significant because the singluar values are much lower than the first two . Matrix factorization methods are extensively used in several use cases . Clustering | Dimensionality Reduction | Product Recommendations | to name a few. . A = np.array([[2,0,3,4],[1,0,2,2],[1,2,0,1],[1,2,0,0],[0,0,1,1]]) . A.shape . (5, 4) . U,S,VT = np.linalg.svd(A) . S . array([6.41687031, 3.03943637, 0.57698562, 0.5026824 ]) . U . array([[-0.83572573, -0.14515127, -0.29646439, 0.26000475, 0.35355339], [-0.46197483, -0.11449508, 0.52286834, 0.00887863, -0.70710678], [-0.19999168, 0.67730305, -0.50049805, -0.35463447, -0.35355339], [-0.08661448, 0.70353837, 0.55121115, 0.26210298, 0.35355339], [-0.20160113, -0.11007421, 0.29049188, -0.85898494, 0.35355339]]) . U_df = pd.DataFrame(U) . U_df . 0 1 2 3 4 . 0 -0.835726 | -0.145151 | -0.296464 | 0.260005 | 0.353553 | . 1 -0.461975 | -0.114495 | 0.522868 | 0.008879 | -0.707107 | . 2 -0.199992 | 0.677303 | -0.500498 | -0.354634 | -0.353553 | . 3 -0.086614 | 0.703538 | 0.551211 | 0.262103 | 0.353553 | . 4 -0.201601 | -0.110074 | 0.290492 | -0.858985 | 0.353553 | . S . array([6.41687031, 3.03943637, 0.57698562, 0.5026824 ]) . VT . array([[-0.37713595, -0.08932896, -0.56612146, -0.72752684], [ 0.32112657, 0.90861676, -0.25482297, -0.07974058], [-0.03353176, 0.17578633, 0.77443073, -0.60682109], [ 0.86805635, -0.3681509 , -0.12177355, -0.31002306]]) . VT_df = pd.DataFrame(VT) . VT_df . 0 1 2 3 . 0 -0.377136 | -0.089329 | -0.566121 | -0.727527 | . 1 0.321127 | 0.908617 | -0.254823 | -0.079741 | . 2 -0.033532 | 0.175786 | 0.774431 | -0.606821 | . 3 0.868056 | -0.368151 | -0.121774 | -0.310023 | . comps = [1,2,3,4,5] for i in range(4): low_rank = U[:, :comps[i]] @ np.diag(S[:comps[i]]) @ VT[:comps[i], :] low_rank_df = pd.DataFrame(low_rank).round(1) print(&quot; n rank %d n&quot; % comps[i]) print(low_rank_df) . rank 1 0 1 2 3 0 2.0 0.5 3.0 3.9 1 1.1 0.3 1.7 2.2 2 0.5 0.1 0.7 0.9 3 0.2 0.0 0.3 0.4 4 0.5 0.1 0.7 0.9 rank 2 0 1 2 3 0 1.9 0.1 3.1 3.9 1 1.0 -0.1 1.8 2.2 2 1.1 2.0 0.2 0.8 3 0.9 2.0 -0.2 0.2 4 0.4 -0.2 0.8 1.0 rank 3 0 1 2 3 0 1.9 0.0 3.0 4.0 1 1.0 0.0 2.0 2.0 2 1.2 1.9 -0.0 0.9 3 0.9 2.0 0.0 0.0 4 0.4 -0.2 0.9 0.9 rank 4 0 1 2 3 0 2.0 0.0 3.0 4.0 1 1.0 -0.0 2.0 2.0 2 1.0 2.0 0.0 1.0 3 1.0 2.0 -0.0 0.0 4 0.0 0.0 1.0 1.0 .",
            "url": "https://srinivasg2306.github.io/blog/2021/07/21/Eigen-Decomposition-SVD-Basics-for-Machine-Learning.html",
            "relUrl": "/2021/07/21/Eigen-Decomposition-SVD-Basics-for-Machine-Learning.html",
            "date": " • Jul 21, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Deep Learning Using Fastai And Pytorch",
            "content": ". I have been going through the book “Deep Learning for Coders with fastai &amp;PyTorch” by Jeremy Howard and Sylvian Gugger. I found this book fascinating, and easy to comprehend. I also found that the code snippets provided in the book, are easy to understand and try out. .   . Deep Learning is full of jargons and complex mathematical formulae. I have tried several top Neural Network books, but halfway through, I used to feel lost. I am practitioner and not an academician who is constantly in touch with calculus and matrix algebra. I am more interested in finding solutions to business problems. I would like to have code/SDK that can be applied to the problems I am trying to solve. It is more of a combination of AI concepts and Applications that I am interested in, not just theory. .   . This book is what I was just looking for! It starts with a note that Deep Learning is indeed not as complicated as it is made to look. And that one does not require a PhD in Data Science or a Degree in Math to build state-of-the-art, novel Deep Learning models. It then goes on to introduce Neural net, and its architecture. It briefly introduces Machine Learning and emphasises on the need to have a holistic view of Deep Learning (including solving for “How will the end-user consume the output?”).  . The Book argues that Jupyter is good enough to build and deploy production grade model, and can be used to build basic Applications with User interface. Most of the Neural Network terms are explained in simple English. For example: Parameters and Activations are just numbers. Activations are the output of each layer of the neural net, and parameters are just coefficients of weights of each independent variable. One concept that caught my mind is universal approximation theorem which states that any function can be approximated to any degree of accuracy by using a combination of 1 Linear and 1 Non-Linear layers of Neural network. This is the basis for Neural Network .   . The book contains examples and sample code for a variety of problems in Cognitive learning space involving text, images and video. At the same time, there are several examples that shows how to use Deep Learning on tabular data to solve problems such ass forecasting, regression and classification. Concepts are explained in simple language, and code snippets are provided at the same time. I think it is very important to be able read and comprehend code written in Python/Pytorch and fastai. It reinforces theoretical concepts that we just learnt. .   . The authors of the book are creators of fastai. Here what the authors have to say about fastai (source: docs.fast.ai) . fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library. fastai includes: .   . A new type dispatch system for Python along with a semantic type hierarchy for tensors . | A GPU-optimized computer vision library which can be extended in pure Python . | An optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4–5 lines of code . | A novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training . | A new data block API . | And much more… . | . fastai is organized around two main design goals: to be approachable and rapidly productive, while also being deeply hackable and configurable. It is built on top of a hierarchy of lower-level APIs which provide composable building blocks. This way, a user wanting to rewrite part of the high-level API or add particular behavior to suit their needs does not have to learn how to use the lowest level. . The examples in the book show how easy it is to build a neural net and deploy it in production - just by writing few lines of codes using fastai on pytorch. .   . There are several documents, and videos that help people at all levels to get onboard with fastai and PyTorch. Here are some useful links . https://www.fast.ai/ . | https://course.fast.ai/ . | https://course19.fast.ai/part2 . | . One of my favourites (and probably yours too, if you want to get deep into the algebra behind neural nets) . https://github.com/fastai/numerical-linear-algebra/blob/master/README.md . | .   . I urge you to give it a try, and see if it helps in expediting some of your Deep learning projects. One note: PyTorch will require GPU machines. Since fastai works on PyTorch, the same applies for fastai too. So make sure you have access to GPU machines. One way is to use Paperspace.com which provides servers with GPU for free. If you have Cloud Credits such as GCP, Azure or AWS, you can spin up machines there and install fastai libraries there. I think AWS and GCP provide fastai images too - which can be deployed on a GPU machine. .   . Another interesting fact is that the book was written on Jupyter notebook. Therefore, free version (jupyter notebook version) is available on fast.ai website. The hardcopy is available on Amazon. .",
            "url": "https://srinivasg2306.github.io/blog/2021/06/27/Deep-Learning-using-fastai-and-PyTorch.html",
            "relUrl": "/2021/06/27/Deep-Learning-using-fastai-and-PyTorch.html",
            "date": " • Jun 27, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Challenges In Setting Up Data And Analytics Practice",
            "content": "Imagine a large manufacturing company that has grown both organically and inorganically. A company that has multiple work cultures, and employs more than 60000 employees. An organization that has multiple IT systems - legacy and modern, and applications generating a lot of data. Has multiple plants and outlets across nation - each one generating even more data every day. Add to that, imagine if the organization is selling multiple brands - each one acting as a separate sub-company complete with its own set of business and IT Teams. Each brand has an army on Analysts - experts in spreadsheet reports. They are experts in pulling out data from ERP and other IT systems, combining and munching the data on their laptops and generating hundreds of spreadsheets for management review. They have no idea of integrated data, data warehousing, Business Intelligence - let alone Advanced Analytics. .   . You are now tasked with setting up Data and Analytics practice that not only simplifies reporting, but drives sales. How do you go about doing it? .   . While a complete solution is out of scope of this blog, I will rather focus on key challenges, and potential pitfalls while setting up an Analytics Organization grounds up. This will help avoid major issues upfront and helps smoother and faster execution of the Project .   . #1: Setting Right Expectations . When you bring in a change, you better let people know exactly what they are going to get. One of the biggest mistakes Organizations do at the start of a Project is “setting incorrect expectations”. . Let me explain. Consider this statement “Brining in a Data platform will automate reporting and insight generation processes”. One can imagine such a statement being made in a conversation between IT and Business Heads, where the IT Head trying to find a sponsor for Data Lake project. The IT head’s intention might have been to explain benefits of a Data platform in layman language, and not necessarily to mislead the Business Head. However, the actual communication of benefits is lost in translation. Data platforms on their own will neither automate reporting nor generate insights. It just facilitates some of these processes. In fact, it is not even necessary to have Data platform to automate reporting or generate insights. . The Business teams should understand that Data platform is a long-term investment, a step in the right direction towards modernization of Data and Analytics Capability. It may not pay off immediately. They will have to understand that more investment is needed in building other supporting platforms such as BI and Analytics to be able to achieve ultimate state - ” Automated Reporting and Actionable insights at Scale that can improve productivity and drive growth “. . So, it is very important that Business leaders get such clarity upfront so that they know what they are getting into. .   . #2: Change Management . According to Wikipedia, Change Management is a collective term for all approaches to prepare, support, and help individuals, teams, and organizations in making organizational change. The complexity of Change management is directly proportional to the size of an Organization. In case of organization such as the aforementioned, Change Management is probably number one Challenge. .   . Change takes time, and managing change involves among other things, educating and training users, Business Process changes, IT system changes, Org structure changes, reshuffling, reskilling and redeployment of employees etc. Change Management has to be carefully thought through and planned . Bringing in Data and Analytics department into the mix will invariably lead to friction with existing IT department which manages Data and Analytics too. Therefore, it is important to clearly define scope of IT , and Data and Analytics Departments. And both the Heads are educated on how to work together as one team. Similarly, automation will invariably take away some manual jobs. People who were previously generating reports manually should now be redeployed, and reskilled. All these activities have to be planned in advance. .   . Never assume that Change management will happen automatically just because there is alignment at Leadership level. It has to be planned meticulously at grassroot level. Each employee should be able to visualize a day in the life after the Change has been implemented. .   . #3: Deliver Value Continuously . Business teams will expect value delivered consistently; they will not wait until a perfect system is established. Target “low hanging fruits” such as productivity improvements through Automation to show value delivered by integrated data, and BI. . Develop a plan to deliver something new and adding incremental value every few weeks. Ensure that these new initiatives are actively adapted by Business and solicit feedback. Identify Business Champions who can drive initiatives. Identify KPIs upfront and track them periodically. . Major Projects deliver large values in a single shot. Such projects should be planned meticulously and managed by dedicated Project Management teams. KPIs must be pre-defined, and expected RoI should be agreed upon . . . #4: Never get stuck when it comes to 3 Ps . People, Processes, and Platforms – These are pillars of an organization. In Data and Analytics domain, these three Ps are in constant flux. Technology is evolving rapidly, and new products and services are being released literally every day! This change in technology has a cascading effect on both Processes and People. For eg: Data Management and Governance processes have changed rapidly in the last few years. Traditional Data warehouses are now making way to Data lakes and Lake houses. Traditional Extract - Transform - Load (ETL) processes are changing to newer plug-and-play ELT. Model selection and tuning, which were primary focus of ML, is making way to ML Operations (DevOps). Analytical models are now commoditized and readily available on code-free platforms .   . A major fallacy when building a new capability is that teams tend to get stuck with day-to-day operations, and do not pay attention to continuous learning and evolving. When learning stops, the team is always in a catch-up mode. The team will lose its ability to innovate and deliver incremental value. Team members will be demotivated because their learning stops, and attrition rates will go up. .   . Therefore, focus on strengthening the 3Ps and never get stuck with day-to-day activities. Continuous learning and innovation are essential ingredients of a successful Organization. .   . #5: Losing sight of Domain Knowledge and Business Processes . This is one of the classic problems with new initiatives driven by IT. The focus tends to be on IT platform, integration, data, scripts, reports etc. However, it is important to note that all these platforms exist to support core business processes. Building a complex system without solid understanding of business processes, may not solve the business problem. On the contrary, a simple automation that reduces bottlenecks may be more useful in addressing business problems. . Domain experts should be part of Analytics team. They are key in formulating Business problem, validating solutions, and ensuring that value is delivered through Data and Analytics. .   . Data and Analytics should always supplement business processes. This is the ultimate goal of any Analytics initiative and should always be borne in minds of practitioners. A lot of Analytics initiatives tend to be driven by Algorithms and models than being borne out a genuine business need. Such initiatives will invariably fail. .   . So, how do we avoid pitfalls and overcome challenges in rolling out Data and Analytics capability? . Build a solid use case and obtain management buy-in. Set expectations very clearly - if possible , narrate a story around a day in life of a stakeholder after changes are implemented. . | Carefully plan and manage the Change process . | Focus on 3Ps and work on continuously improving them. . | Deliver incremental values continuously to keep business invested in Data and Analytics . | Always Align with Business processes - use business process KPIs to measure impact of Data and Analytics . | .",
            "url": "https://srinivasg2306.github.io/blog/2021/05/15/Challenges-in-Setting-up-Data-and-Analytics-Practice.html",
            "relUrl": "/2021/05/15/Challenges-in-Setting-up-Data-and-Analytics-Practice.html",
            "date": " • May 15, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Building Successful Machine Learning Products",
            "content": "Building Successful ML Products . In 2019 Gartner predicted 80% of AI projects will remain as experiments run by individuals. This is one of the key reasons why AI/ML Projects fail. Other reasons such as lack of data, talent, platforms, or leadership buy-in are more fundamental in nature, requiring a lot of time and effort to resolve. But once all the hard work is done, failing to operationalize models is undoing all hard work – management might get frustrated and stop sponsoring ML initiatives. .   . Machine learning projects typically start with a problem statement: A specific problem which can be solved using data and analytics. Data is collected and studied, and basic exploratory analysis is done. Data Scientists then run experiments and build models and train them. Upon validation, the model is frozen and outputs are generated. Most projects stop at this stage, and every time there is a need to generate output, the models are run manually .   . There are three major issues with this approach: . Lack of Scalability: Running models manually on local machines will not scale because of limitations in the size of machine. Also, as the scope of the model increases, size of data and steps involved in generating output increases, and hence running the steps manually becomes time consuming and infeasible . | Lack of Process Integration: The end objective of any analytics project is to engrain analytics in business process/applications. Therefore, the whole end-to-end workflow, and integration has to be thought through upfront. The risk of model never being used is higher if workflow and integration become afterthought. . | Model Deterioration: Any analytical model requires maintenance and constant monitoring of output. The quality of predictions deteriorate over a period of time. Model deterioration typically happens because of data drift (Changes in Data pattern over a period of time). . |   . In order to overcome these issues, and increase chances of success, it is recommended to follow a product driven approach to analytics project. The end goal of a product driven approach is to build an Analytic product which is fully integrated with core applications, and something that goes through product lifecycle itself. .   . There are several approaches to build ML products and one such popular approach is the Drivetrain Approach https://www.oreilly.com/radar/drivetrain-approach-data-products/ . Drive train approach is a top-down approach which is driven by business objectives. It seeks to add value to business processes by bringing data and insights into the process flow. Model building is just ‘ another brick in the wall’. Rather than being model centric, this approach strives to be Process and Data centric. Another important aspect of this approach is simulating the end result and the workflow. This is lacking in most of model centric approaches where the entire focus is on algorithms and accuracy metrics of the model. Drive-train approach provides equal weightage to . Business Process improvements . | Application Integration . | Ease of use and consumption . | Models . | MLOps . |   . The following is an extension of Drivetrain approach focusing on Analytical Product Design and Maintenance . Step 1: Clearly define an objective of the Product. Eg: Recommendation engine that can drive sales by surprising and delighting customers. .   . Key Tasks . Understanding Business Challenges and Key Business Processes impacted by the product . | AS-IS process . | Brainstorming ideas with Key Business Stakeholders (why do we need this product? What is the benefit? etc) . | Phrasing Objective Statements and sign off . | .   . Tools (Platform) . MS Word/ Excel/Ppt | .   . Step 2: Simulating the product: Identify workflows, inputs/outputs, interfaces, simulate end result, how will the outputs be consumed? Who are the consumers? How frequently will the product refresh present it to business obtain sign off .   . Key Tasks . Identify Business Processes impacted . | Map process flow and identify Users(Actors), Inputs, Outputs, Processes . | Identify Analytics product touch points with the processes . | Develop Analytics product workflow (Model building inferencing Application integration) . | Define product SLAs - response time etc . | Define inferencing strategy (batch or real-time) . | Define UI/UX where applicable (Eg: process flow which requires users to upload data or trigger a run) . | .   . NOTE: At this point we have still not defined Model or improvements brought by it .   . Tools (Platform) . MS Word/ Excel/Ppt . | Visio . | Jupyter notebook . | . Step 3: Identify levers of the product - What is the USP of the product? Eg: Ranking of recommendations, Sharp recommendations - at fit and size level with CTA and images of the product. . Key Tasks . Define “Improvements” to be brought to the process very clearly. What constitutes significant improvement to existing process? What are the levers that can be used? . | What is the USP of this product? . | .   . Tools (Platform) . MS Word/ Excel/Ppt | . Step 4: Gather relevant data needed to bring the improvements . Define what data are needed . | Identify Data Sources and Gather them (if data is not available see if they can be sourced afresh) . | Data Preparation . | EDA . | Insights . | Freeze all Data requirements, and Preparation . | .   . Tools /Platform . MS Word/ Excel/Ppt . | SQL Workbench . | Jupyter notebook (Local machine / Azure ML) . | Azure ML . | .   . Step 5 Modelling - Select relevant models, experiment with multiple models . Select candidate models . | Simulate input and outputs of the model . | Prepare data for inputting into the model . | Start Experimentation . Feature extraction . | Feature engineering . | Model building . | Model testing and validation . | Model selection . | . | .  Tools /Platform . SQL Workbench . | Jupyter notebook (Local machine / Azure ML) . | Azure ML . | .   . Step 6: MLOps . Deploy Pipelines to production . | Create Inferencing end points and share them with consuming applications . | Testing . | Schedule model run frequency . | Deploy model quality monitor and alerting mechanism . | Access control . | . Tools /Platform . Azure ML | .   . Step 7: Complete Product Development and Rollout . Build UI of the product (Eg: interface for uploading data, or triggering a model run) . | Build Dashboards . | Deploy Solutions and roll out product . | .   .  Tools /Platform . Azure ML . | App development platform (for UI/UX , if required) . | Jupyter notebook . | .",
            "url": "https://srinivasg2306.github.io/blog/2021/05/03/Building-Successful-Machine-Learning-Products.html",
            "relUrl": "/2021/05/03/Building-Successful-Machine-Learning-Products.html",
            "date": " • May 3, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Analytics Course Done Now What",
            "content": "I completed Analytics and Business Intelligence program at IIMB in 2019. This is a one-year program which is pretty involved. It starts with fundamentals of Statistics, and covers advanced topics including Deep learning and Reinforcement learning. The program is pretty comprehensive and well thought out and is useful for people who want to move into Analytics field, or would like to advance their career in Analytics. . The course is focussed on concepts rather than tools or Operationalization of Analytics. It does not cover Cloud Platforms either. For more information about the course visit https://iimb.ac.in/eep/product/259/Business-Analytics-Intelligence .   . Coming from Data background, my primary objective of taking the course was to get myself familiarized with Data Science, ML and DL, and get some hands-on experience in Python, R and other popular tools in advanced analytics. . Of course, I did realize that the course would not equip me with everything that is required to jump start an analytics career. However, it was a good place to start with, and given my experience in Data Engineering, the course was a nice complement to my existing skill set. The course helped me switch job and move into Head of Analytics role, which was one of my key objectives, and it was fulfilling to have found a new job while still doing the course .   . 6 months into my new job, I realized that real-life challenges are not really about Data Science, Algorithms or selection of models. I noted that i could not simply put my knowledge and skillsets gained during the period of the Course to use. . My first challenge was to get Data. Sourcing, Profiling, Curating, Storing and then Governing data was my first challenge. This took nearly an year to sort out. Organizations go through Analytics journey in phases. First phase is the Data Phase - Consolidating Organizational level data into a central repository, Second Phase is BI: Doing basic descriptive Analytics, and Business intelligence to help in decision making, and monitoring KPIs. Third phase: Advanced Analytics: Predictive Analytics, Prescriptive Analytics and Final Phase where Data, BI and Advanced Analytics are seamlessly integrated and a stable platform is established, and robust delivery processes such as MLOps are established. .   . I found myself at the beginning of Phase 1 of the journey, but my course was relevant to Phase 3 or 4. The challenge was how to leverage my skill sets in the present scenario? I suppose many of my batch mates found themselves in similar situations. And in general a large number of organizations are in early stages of their analytics journey where the focus tends to be on Data and BI. And fair enough because these are low hanging fruits, and provide highest value to the organizations. .   . So, what now? I had two options - just focus on Data and BI only, and take up advanced analytics at a later time when both Data and BI are in stable state, and advanced. This would mean that there will be no advanced analytics project for next 2 - 3 years. The other option is to take baby-steps in advanced analytics while accelerating data and BI improvements. This will then take couple of years by which time Advanced Analytics practiced would have matured to a certain level. So, I hired a Data Scientist and set up a basic windows server with R, and Python installed in it. Data Scientists were now able to analyse data, and build basic models. .   . In my next blog, i will talk about how did things go? What went right and what went wrong? What are some of the things we should have done upfront before setting up Analytics practice etc. .",
            "url": "https://srinivasg2306.github.io/blog/2021/05/02/Analytics-Course-Done-Now-What.html",
            "relUrl": "/2021/05/02/Analytics-Course-Done-Now-What.html",
            "date": " • May 2, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "About P Values And Hypothesis Testing",
            "content": "There are two contrasting views of Hypothesis testing. There is a group which is of the view that Hypothesis testing is essential for establishing statistical significance of a metric, is an essential part of Model validation. There is another group which believes that Hypothesis does not really prove or disprove a theory, but merely measures data sufficiency to arrive at any conclusion. For now, let us assume that Hypothesis testing is useful, and narrate an example . Marketing Campaigns are done in the form of sending messages from a company to its customers with an objective of increasing sales. It is believed that when messages are sent, some of the customers see them. Few among them will be driven to make a purchase, typically, over a period of time. Messages are sent to customers, and their purchases are tracked for a period of time. Any sale happening during the period is attributed to the messages sent during that period. . (it is assumed that there is only one marketing message during the period, if there are multiple messages then the sale is attributed either to the first or last message) .   . The attribution process is not fool-proof, and is driven by a lot of theory than actual facts. It is hard to believe that an SMS can drive crores of sales (RoI is absurdly high in these cases). In reality, the reasons can be different - Offers, Events like Birthday wedding etc., or any unknown needs of customers. While it is difficult to predict why a customer buys, it is possible to trigger a need for purchase in a customer by sending a communication at right time. .   . Nevertheless, it is important to have a strong measure for lift and attribution in order to establish whether a marketing campaign works or not. One way of measuring effectiveness of campaign is by creating test and control groups. So how do we create test and control groups? To understand this, we need to know the meaning of “Strength of Test”. The article below gives good information - . https://www.markhw.com/blog/control-size . The strength of test is highest when sample sizes are equally split among test and control, and rapidly diminishes when control group goes below 20%. Depending on the total size of the sample set, any number between 50-20% would be ideal. .   . How to measure statistical significance of lift? . We run 2 sample Z test for proportion to check Significance . H0: Response rate of Test Group = Response rate of Control Group (There is no significance difference in response rates between Test and Control Groups) . Ha : Response rate of Test Group is not equal to Response rate of Control Group .   .   . Z = (P1-P2)/(sqrt(P(1-P)*(1/N1 + 1/N2)) . P1= response proportion in test group . P2 = response proportion in control group . N1 = size of test group . N2 = size of control group . P = (N1*P1 + N2*P2)/(N1+N2) .   . The value of Z for two tailed test at 95% significance level is 1.96. That is - there is a 95% chance that you will retain null hypothesis when it is true, if your Z statistic is 1.96 or more. . P value - chances of observing the test statistic value given that the null hypothesis is true. P value should be as low as possible for us to be confidently reject null hypothesis. It should definitely be &lt; 0.05 base minimum for us to reject null hypothesis .   . You can run an experiment keeping P1, P2 constant but increasing N1 and N2. You will see that the Z value increases even if you increase the numbers in the original proportion of N1 and N2. This clearly indicates that as the size of data grows even if the proportion of test and control group remains same, the significance increases. . Similarly, you can keep N1 and N2 constant and vary values of P1 and P2 such that P1-P2 increases, in this case, as P1-P2 increases, the significance value increases . In conclusion, Hypothesis tests can be used to check sufficiency of data to make any conclusion about the Hypothesis. It never provides a conclusive evidence supporting a Hypothesis. Nevertheless, it helps in determining if observations are entirely random or due to interaction between certain variables. .",
            "url": "https://srinivasg2306.github.io/blog/2021/05/02/About-p-values-and-Hypothesis-Testing.html",
            "relUrl": "/2021/05/02/About-p-values-and-Hypothesis-Testing.html",
            "date": " • May 2, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a Data and Analytics Enthusiast. Although most part of my career was spent building Mainframe Applications, I discovered that my passion was really in Math, Stats, Data and Analytics. I just love progaming in Python, and try to stay in touch with latest in the field. I consider myself a student and blogging is one of the best way to learn and review concepts. So here I am!! . Here is my Linkedin Profile . https://www.linkedin.com/in/srinivas-gopinath-7948593a/ .",
          "url": "https://srinivasg2306.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://srinivasg2306.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}