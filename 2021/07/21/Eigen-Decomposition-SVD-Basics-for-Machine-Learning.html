<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Eigen Decomposition Svd Basics For Machine Learning | The Data and Analytics Blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Eigen Decomposition Svd Basics For Machine Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Concept of Vectors and Matrices" />
<meta property="og:description" content="Concept of Vectors and Matrices" />
<link rel="canonical" href="https://srinivasg2306.github.io/blog/2021/07/21/Eigen-Decomposition-SVD-Basics-for-Machine-Learning.html" />
<meta property="og:url" content="https://srinivasg2306.github.io/blog/2021/07/21/Eigen-Decomposition-SVD-Basics-for-Machine-Learning.html" />
<meta property="og:site_name" content="The Data and Analytics Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-07-21T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://srinivasg2306.github.io/blog/2021/07/21/Eigen-Decomposition-SVD-Basics-for-Machine-Learning.html","@type":"BlogPosting","headline":"Eigen Decomposition Svd Basics For Machine Learning","dateModified":"2021-07-21T00:00:00-05:00","datePublished":"2021-07-21T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://srinivasg2306.github.io/blog/2021/07/21/Eigen-Decomposition-SVD-Basics-for-Machine-Learning.html"},"description":"Concept of Vectors and Matrices","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://srinivasg2306.github.io/blog/feed.xml" title="The Data and Analytics Blog" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">The Data and Analytics Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Eigen Decomposition Svd Basics For Machine Learning</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-07-21T00:00:00-05:00" itemprop="datePublished">
        Jul 21, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      9 min read
    
</span></p>

    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="concept-of-vectors-and-matrices">Concept of Vectors and Matrices</h2>

<p>Mathematically, A vector is represented by a single dimension array of numbers. Example: V = [25, 3, 46, 30, 33]. This makes no sense to a Machine Learning practitioner until it can be related to something real. Let us say the numbers represents certain data of an individual - Age, years of Work experience, Annual Salary(in 000 USD), Average Expenses per year (in 000 USD), % of Annual salary saved per year. If such data are collected for multiple individuals and stacked up one over the other, then we have an array of vectors which is called a <em>Matrix</em>.</p>

<p>From a Linear Algebra perspective, a Matrix can represent one of the two:</p>

<ol>
  <li><strong>Systems of Linear Equations</strong> - Matrix operations can be used to represent a set of linear equations. This helps us solve the equations, define inverse of matrices.</li>
  <li><strong>Linear Mapping</strong> - Matrices can be used to define Linear Mappings. Linear mapping define transformation actions that can be performed on Vectors. Examples of Vector Transformations that fall into this bucket are: Rotation, Stretching along specific axes, change of basis</li>
</ol>

<h2 id="matrix-decomposition">Matrix Decomposition</h2>

<p>Let us take an example of a Matrix that represents a set of Vectors - i.e . Data. Let us assume that the Data consists of rows - which represents users, and columns which represents products that they have purchased. So we have a user-product matrix. The matrix maps each user to each product. If a user Ui has purchased product Vj then the cell ij will contain the number of items purchased. If a user has never purchased a product, then the corresponding cell will have a 0. Please note that at this stage, we just have Sales Data arranged in a certain way (in the form of matrix). This does not give any additional information. <strong><em>So how do we break it down into chunks of comprehendable pieces? Enter Factorization!</em></strong></p>

<h3 id="concept-of-matrix-factorization">Concept of Matrix Factorization</h3>

<p>Suppose you had a number - say 28, and you want to understand it better. What do you do? You can say 28 is 7 times 4, which is 7 times 2 times 2 (7 x 2 x 2). This tells us that 7 and 2 are latent in the number 28. What happens if we apply factorization to Matrices? By extending the logic, we must be able to understand the latent factors of matrices. The process of decomposing a matrix into certain factors of interpretable matrices is known as Matrix Factorization or Matrix Decomposition</p>

<h3 id="matrix-decomposition-methods">Matrix Decomposition Methods</h3>

<p>There are two ways to decompose matrices - <strong><em>Eigen Decomposition, Singular Value Decomposition</em></strong>. Both methodologies are similar. The fundamental difference is that Eigen decomposition can only be done on square matrices. Whereas Singular Value Decomposition can be done on any matrix and is generic in a sense. Regardless, at the core of Matrix Decomposition method is identifying values that capture key information about the parent matrix.</p>

<h4 id="eigen-decomposition">Eigen Decomposition</h4>

<p>Let A =</p>

<p><img src="https://srinivasg2306.github.io/blog/assets/img/2021-07-21-Eigen-Decomposition-SVD-Basics-for-Machine-Learning/media/image1.tmp" alt="" /></p>

<p>A Square matrix can be decomposed into Eigen values and Eigen vectors as follows</p>

<ul>
  <li>Let X1, X2, X3…Xn be Eigen Vectors of a matrix A, such that<br />
AX1 = LX1, AX2 = LX2,…AXn = LXn where L1, L2, L3,..Ln are Eigen values</li>
  <li>Then Matrix P = [X1,X2,X3…Xn] where Vectors X1, X2..Xn arranged in a columnar fashion, D = Diagonal matrix of [L1,L2,..Ln], P<sup>-1</sup>- inverse of P</li>
  <li>P is matrix of eigen vectors, D is eigenvalues arranged in a diagonal matrix, and P<sup>-1</sup> which is the Inverse of P</li>
  <li>It can be Mathematically proven that A = P.D.P<sup>-1</sup>(Dot products)</li>
  <li>Eigen values and Eigen vectors capture information about a matrix. The first eigen vector and first eigen value captures highest information about the matrix. Similarly, second eigen vector and second eigen value captures 2nd highest information so on…</li>
  <li>This technique is useful in dimensional reduction/compression</li>
</ul>

<p>Post Eigen decomposition of A, we have the following</p>

<p><img src="https://srinivasg2306.github.io/blog/assets/img/2021-07-21-Eigen-Decomposition-SVD-Basics-for-Machine-Learning/media/image2.tmp" alt="" /></p>

<p>An illustration of Eigen Values and Eigen Vectors in action - Google Page Rank Algorithm.</p>

<p>Here is the URL to a page with a neat illustration of how Eigen values work <a href="http://pi.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture3/lecture3.html">http://pi.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture3/lecture3.html</a></p>

<h4 id="code">Code</h4>

<table>
<tbody>
<tr class="odd">
<td><pre><code>import numpy as np
import pandas as pd

A = np.array([[1,2,3],[3,2,7],[6,5,4]])

D,P = np.linalg.eig(A)

P
array([[-0.32703016, -0.56731925, -0.05408783],
       [-0.63306085,  0.81351676, -0.7949926 ],
       [-0.70163042, -0.12782546,  0.60420301]])

P_inv = np.linalg.inv(P)

P_inv

array([[-0.56004896, -0.50227981, -0.71102023],
       [-1.35059073,  0.33832307,  0.32425164],
       [-0.93608812, -0.51169638,  0.89799982]])

D
array([11.30795739, -1.19198827, -3.11596912])</code></pre></td>
</tr>
</tbody>
</table>

<h4 id="singular-value-decomposition">Singular Value Decomposition</h4>

<p>Okay, Eigen decomposition works when the matrix is a square matrix. But in most real-life scenarios, the matrices are not square. Therefore, Eigen decomposition is not useful in many cases. So, what do we decompose non-square matrices? Singular Vector Decomposition is the answer to this problem.</p>

<p>Any Matrix A<sub>mxn</sub> can be decomposed as</p>

<p>A<sub>mxn</sub> = U<sub>mxm</sub>.S<sub>mxn</sub>.V<sup>T</sup><sub>nxn</sub></p>

<p>Where</p>

<ul>
  <li>S - Singular matrix which is a diagonal matrix</li>
  <li>U - Left Singular matrix</li>
  <li>V<sup>T</sup>- Right Singular matrix</li>
</ul>

<p>The diagonal values in Matrix S is called Singular Matrix and the Singular values arranged in the descending order from top-left</p>

<p>SVD is used in a number of ways in Machine learning including solving systems of linear equations, curve fitting etc. Substituting a matrix with its SVD approximation reduces calculation as well as loss. Here, we describe how this is achieved Let A be a user item matrix (4 users and 5 items) as shown below. The Values in the table describe the quantities purchased</p>

<p>Let us see SVD in action through an Example</p>

<p>Let A be a user item matrix (4 users and 5 items) as shown below. The Values in the table describe the quantities purchased</p>

<table>
  <thead>
    <tr>
      <th>Cat/User</th>
      <th>U1</th>
      <th>U2</th>
      <th>U3</th>
      <th>U4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Formal_Shirt</strong></td>
      <td>2</td>
      <td>0</td>
      <td>3</td>
      <td>4</td>
    </tr>
    <tr>
      <td><strong>Trouser</strong></td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <td><strong>T_shirt</strong></td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <td><strong>Jeans</strong></td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td><strong>Blazer</strong></td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<p>Let us get started with Decomposition</p>

<p><img src="https://srinivasg2306.github.io/blog/assets/img/2021-07-21-Eigen-Decomposition-SVD-Basics-for-Machine-Learning/media/image3.tmp" alt="" /></p>

<p>SVD Decomposition results in the following matrices</p>

<p><img src="https://srinivasg2306.github.io/blog/assets/img/2021-07-21-Eigen-Decomposition-SVD-Basics-for-Machine-Learning/media/image4.tmp" alt="" /></p>

<p>Let us calculate the first low rank matrix (Reconstructing)</p>

<p>U<sub>1</sub>.S<sub>1</sub>.V<sub>1</sub><sup>T</sup></p>

<p><img src="https://srinivasg2306.github.io/blog/assets/img/2021-07-21-Eigen-Decomposition-SVD-Basics-for-Machine-Learning/media/image5.tmp" alt="" /></p>

<p>If you add U<sub>2</sub>.S<sub>2</sub>.V<sub>2</sub><sup>T</sup> to Rank 1 approximation then we get Rank 2 Matrix</p>

<p><img src="https://srinivasg2306.github.io/blog/assets/img/2021-07-21-Eigen-Decomposition-SVD-Basics-for-Machine-Learning/media/image6.tmp" alt="" /></p>

<p>This is already close to the original matrix. Rank, 3 and 4 will of course add more information to it. However, from computation perspective, it is much easier to multiply 5x2, 2x2, and 2x4 matrices than trying to crunch the original 5x4 matrix</p>

<p>How does this reduce computation? Note that a matrix of Size mxn is replaced with a smaller matrices of mxp, pxn- where p is rank of the singular matrix. P can also be seen as the number of latent factors</p>

<p>In our example we saw 5x4 matrix being replaced with one 5x2 matrix and another 2x4 matrix in addition to two real numbers. In real-life situations, matrices of several thousands of rows and columns can be reduced to ones with few hundreds. Storing such matrices require less memory and information is retained as well.</p>

<p>The Rank P approximation can also be related to the number of latent factors. In fact, the Rank P is also equal to the number of latent factors. Let us delve a little deeper into the example we have taken.</p>

<p>In Matrix U above, the first cell of the first column (-0.83) indicates that <strong>formal_shirt</strong> is unique and has its own factor. Similarly, the 3rd and 4th cell in the second column are close to each other and have relatively large values. These values correspond to Jeans and T-Shirts. <strong>This indicates that Jeans and T-Shirts are part of same cluster (most likely casual clothing).</strong> The other columns, even though giving some information about how products might be related, are not significant because the singular values are much lower.</p>

<p>Similarly in Matrix V<sup>T</sup> Users 3,4 are close to each other in their tastes (Some one that buys formals and casuals) as shown in the third and fourth cells of 1st row. User 2 is unique in his taste and hence has a value of 0.90 and no other users are close to this user as indicated in 2nd cell of Row 2 in the Matrix. The last two rows, although, giving some information about the users, are not significant because the singluar values are much lower than the first two</p>

<p>Matrix factorization methods are extensively used in several use cases</p>

<ol>
  <li>Clustering</li>
  <li>Dimensionality Reduction</li>
  <li>Product Recommendations</li>
</ol>

<p>to name a few.</p>

<h4 id="code-1">Code</h4>

<table>
<tbody>
<tr class="odd">
<td><pre><code>A = np.array([[2,0,3,4],[1,0,2,2],[1,2,0,1],[1,2,0,0],[0,0,1,1]])

U,S,VT = np.linalg.svd(A)

S
array([6.41687031, 3.03943637, 0.57698562, 0.5026824 ])
U
array([[-0.83572573, -0.14515127, -0.29646439,  0.26000475,  0.35355339],
       [-0.46197483, -0.11449508,  0.52286834,  0.00887863, -0.70710678],
       [-0.19999168,  0.67730305, -0.50049805, -0.35463447, -0.35355339],
       [-0.08661448,  0.70353837,  0.55121115,  0.26210298,  0.35355339],
       [-0.20160113, -0.11007421,  0.29049188, -0.85898494,  0.35355339]])

U_df = pd.DataFrame(U)
U_df
          0         1         2         3         4
0 -0.835726 -0.145151 -0.296464  0.260005  0.353553
1 -0.461975 -0.114495  0.522868  0.008879 -0.707107
2 -0.199992  0.677303 -0.500498 -0.354634 -0.353553
3 -0.086614  0.703538  0.551211  0.262103  0.353553
4 -0.201601 -0.110074  0.290492 -0.858985  0.353553

S
array([6.41687031, 3.03943637, 0.57698562, 0.5026824 ])

VT
array([[-0.37713595, -0.08932896, -0.56612146, -0.72752684],
       [ 0.32112657,  0.90861676, -0.25482297, -0.07974058],
       [-0.03353176,  0.17578633,  0.77443073, -0.60682109],
       [ 0.86805635, -0.3681509 , -0.12177355, -0.31002306]])

VT_df = pd.DataFrame(VT)
VT_df
          0         1         2         3
0 -0.377136 -0.089329 -0.566121 -0.727527
1  0.321127  0.908617 -0.254823 -0.079741
2 -0.033532  0.175786  0.774431 -0.606821
3  0.868056 -0.368151 -0.121774 -0.310023

comps = [1,2,3,4,5]

for i in range(4):
        low_rank = U[:, :comps[i]] @ np.diag(S[:comps[i]]) @ VT[:comps[i], :]
        low_rank_df = pd.DataFrame(low_rank).round(1)
        print(&quot;\n rank  %d \n&quot; % comps[i])
        print(low_rank_df)
       

 rank  1 

     0    1    2    3
0  2.0  0.5  3.0  3.9
1  1.1  0.3  1.7  2.2
2  0.5  0.1  0.7  0.9
3  0.2  0.0  0.3  0.4
4  0.5  0.1  0.7  0.9

 rank  2 

     0    1    2    3
0  1.9  0.1  3.1  3.9
1  1.0 -0.1  1.8  2.2
2  1.1  2.0  0.2  0.8
3  0.9  2.0 -0.2  0.2
4  0.4 -0.2  0.8  1.0

 rank  3 

     0    1    2    3
0  1.9  0.0  3.0  4.0
1  1.0  0.0  2.0  2.0
2  1.2  1.9 -0.0  0.9
3  0.9  2.0  0.0  0.0
4  0.4 -0.2  0.9  0.9

 rank  4 

     0    1    2    3
0  2.0  0.0  3.0  4.0
1  1.0 -0.0  2.0  2.0
2  1.0  2.0  0.0  1.0
3  1.0  2.0 -0.0  0.0
4  0.0  0.0  1.0  1.0
</code></pre></td>
</tr>
</tbody>
</table>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code></code></pre></div></div>

  </div><a class="u-url" href="/blog/2021/07/21/Eigen-Decomposition-SVD-Basics-for-Machine-Learning.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A Practitioners Perspective of Data and Analytics</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/srinivasg2306" title="srinivasg2306"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/srinivasg2306" title="srinivasg2306"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
